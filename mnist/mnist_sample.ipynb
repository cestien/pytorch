{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Jeremy Howard, fast.ai.*\n",
    "\n",
    "In this notebook we'll see how easy it is to use the simple functions we created along with `torch.nn` and friends to train MNIST. We're going to create the same convolution net that we created at the end of the previous notebook, but this time we'll use the functions we've already written, and we'll skip the explanatory text so you can see just the final code.\n",
    "\n",
    "You can use this same notebook to train other neural nets on other datasets with minimal changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modulo `mnist_sample.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn,optim,tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pickle, gzip\n",
    "\n",
    "\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        for xb,yb in train_dl: \n",
    "            pb = model(xb)\n",
    "            loss = loss_func(pb,yb)  \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        losses = list()\n",
    "        nums = list()\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in valid_dl:\n",
    "                pb = model(xb)\n",
    "                loss  = loss_func(pb,yb)\n",
    "                num = len(xb)\n",
    "                losses.append(loss.item())\n",
    "                nums.append(num)\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WrappedDataLoader():\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self): return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches: yield(self.func(*b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del módulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
    "    ((train_x, train_y), (valid_x, valid_y), _) = pickle.load(f, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "lr=0.1\n",
    "epochs=4\n",
    "\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y): return x.view(-1,1,28,28).to(dev),y.to(dev)\n",
    "\n",
    "def get_dataloader(x,y,bs,shuffle):\n",
    "    ds = TensorDataset(*map(tensor, (x,y)))\n",
    "    dl = DataLoader(ds, batch_size=bs, shuffle=shuffle)\n",
    "    for xb, yb in dl:\n",
    "        xp = xb.view(-1,1,28,28).to(dev)\n",
    "        yp = yb.to(dev)\n",
    "    return xp,yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_dataloader(train_x, train_y, bs,   shuffle=False)\n",
    "valid_dl = get_dataloader(valid_x, valid_y, bs*2, shuffle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic custom layer\n",
    "class Custom_layer(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1,  16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Custom_layer(lambda x: x.view(x.size(0),-1))\n",
    ").to(dev)\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fit(epochs, model, loss_func, opt, train_dl, valid_dl)\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_func, opt, train_dl, valid_dl)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Entrenamiento\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb,yb \u001b[38;5;129;01min\u001b[39;00m train_dl: \n\u001b[1;32m     15\u001b[0m         pb \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m     16\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_func(pb,yb)  \n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
