{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estructura de red convolucional + recurrente para LATINO40, con CTC\n",
    "\n",
    "Este programa está basado en [Building an End-to-End Speech Recognition Model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/), por Michael Nguyen. Es una implementación de la red conocida como [Deep Speech 2](https://arxiv.org/abs/1512.02595), con ligeras variantes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo es: \n",
    "\n",
    "- Implementar el modelo de la red (convolucional + recurrente)\n",
    "- Entrenar utilizando la loss CTC, que contempla todos los alineamientos posibles. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y dataloader.\n",
    "\n",
    "Ahora ya no existe más el problema del alineamiento porque ese alineamiento se hace dentro de la función de costo:\n",
    "\n",
    "<img src=\"figs/loss_CTC.png\" alt=\"loss CTC\" width=\"700\"/>\n",
    "\n",
    "Los *datos* tienen la dimensión de las columnas del Mel-espectrograma, o cantidad de frames, y también dependerá del stride. En cambio, si los *objetivos* son los caracteres, no van a coincidir las cantidades que tengo de uno y otro. Esto en el anterior programa vimos de hacer un estiramiento a mano. La loss CTC lo hace automático. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# El dataset es el mismo\n",
    "class Latino40Dataset(data.Dataset):\n",
    "    def __init__(self, annotations_file, data_root):\n",
    "        with open(annotations_file) as json_file:\n",
    "            data_dict = json.load(json_file)\n",
    "        self.annotation = list(data_dict.values())# convertir los valores a lista me permite indexarlos\n",
    "        self.data_dir = data_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)# largo de la lista\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_name = self.annotation[idx]['wav'].replace('{data_root}',self.data_dir)\n",
    "        waveform, sample_rate = torchaudio.load(wav_name)\n",
    "        label = self.annotation[idx]['words']\n",
    "        return waveform, sample_rate, label\n",
    "    \n",
    "\n",
    "train_dataset = Latino40Dataset(\"./data/latino40_split/train.json\",\"./data\")# Esta es la llamada a Latino40Dataset.__init__\n",
    "test_dataset = Latino40Dataset(\"./data/latino40_split/valid.json\",\"./data\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mantenemos las misma `collate_fn`, que cargará las palabras, las convierte en caracteres y genera un alineamiento de longitud uniforme. Por último convierte los caracteres en clases (numérico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La transformación de los labels es la misma\n",
    "char_map_str = \"\"\"  \n",
    " <SPACE> 0\n",
    " a 1\n",
    " b 2\n",
    " c 3\n",
    " d 4\n",
    " e 5\n",
    " f 6\n",
    " g 7\n",
    " h 8\n",
    " i 9\n",
    " j 10\n",
    " k 11\n",
    " l 12\n",
    " m 13\n",
    " n 14\n",
    " o 15\n",
    " p 16\n",
    " q 17\n",
    " r 18\n",
    " s 19\n",
    " t 20\n",
    " u 21\n",
    " v 22\n",
    " w 23\n",
    " x 24\n",
    " y 25\n",
    " z 26\n",
    " \\u03B5 27\n",
    "\"\"\" #una sola string que contiene todo el mapeo. \\u03B5 es el epsilon\n",
    "\n",
    "# Clase TextTransform: mapea de caracter a entero y viceversa\n",
    "# La función init de esta clase va a generar dos diccionarios que mapean de caracter a entero y viceversa\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        # char_map = char_map_str\n",
    "        self.char_map = {}# diccionario para mapear de caracter a entero\n",
    "        self.index_map = {}# diccionario para mapear de entero a caracter\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[0] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Usa el char_map y convierte una secuencia de caracteres a una secuencia de enteros \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Usa el index_map y convierte una secuencia de enteros a una secuencia de caracteres\"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[int(i)])\n",
    "        return ''.join(string).replace('', ' ')\n",
    "\n",
    "texttransform = TextTransform() # Acá se ejecuta el init de la clase TextTransform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ppelle/anaconda3/envs/speechbrain/lib/python3.8/site-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "\n",
    "# La función de transformación de audio para train y test es la misma\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    T.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft=512, win_length=400),\n",
    "    T.FrequencyMasking(freq_mask_param=15),\n",
    "    T.TimeMasking(time_mask_param=35)\n",
    ")# aquí se inicializa la transformación de audio para train \n",
    "\n",
    "valid_audio_transforms = T.MelSpectrogram(sample_rate=16000, n_mels=128, n_fft=512, win_length=400)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora no es necesario expandir nada, los alineamientos los resuelve la función de costo (como era originalmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(TextTransform().text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0]//2)\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a testearlo con un batch de 3 elementos\n",
    "batch_size = 3\n",
    "tmp_loader = data.DataLoader(dataset=test_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: data_processing(x, 'test'))\n",
    "\n",
    "tmp_it = iter(tmp_loader)\n",
    "x,lab,x_length,lab_length = next(tmp_it)\n",
    "print('waveforms.shape {}'.format(x.shape))\n",
    "print('Etiquetas_batched.shape {}'.format(lab.shape))\n",
    "print('Largo del batch de specgrams {}'.format(x_length))\n",
    "print('Largo del batch de etiquetas {}'.format(lab_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "waveform, _ , _ = test_dataset[0]\n",
    "print('Waveform shape {}'.format(waveform.shape))\n",
    "spectrogram,_,input_length,_ = data_processing([train_dataset[0]])\n",
    "print('Spectrogram shape: {}'.format(spectrogram.shape))\n",
    "print('Según los parámetros elegidos en la transformación el hop_size = nwin //2, es decir {}'.format(400//2))\n",
    "print('La longitud del espectrograma sería {} (ni ahí)'.format(waveform.shape[1]//200))\n",
    "spec = spectrogram.squeeze().data.numpy()\n",
    "[fig,ax] = plt.subplots()\n",
    "\n",
    "ax.pcolor(np.log(spec+1e-10))\n",
    "ax.set_title(label='Espectrograma de la frase: {}'.format(texttransform.int_to_text(lab[0].numpy())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "        \n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \"\"\"Speech Recognition Model Inspired by DeepSpeech 2\"\"\"\n",
    "\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo con un batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"n_cnn_layers\": 2,\n",
    "    \"n_rnn_layers\": 1,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 28,\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"batch_size\": batch_size,\n",
    "}\n",
    "\n",
    "model1 = SpeechRecognitionModel(\n",
    "    hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "    hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "    )\n",
    "\n",
    "print(model1)\n",
    "print('Num Model Parameters', sum([param.nelement() for param in model1.parameters()]))\n",
    "\n",
    "x1,lab1,x1_length,lab1_length = next(tmp_it)\n",
    "print('waveforms.shape {}'.format(x.shape))\n",
    "print('Etiquetas_batched.shape {}'.format(lab1.shape))\n",
    "y1 = model1(x1)\n",
    "print('Output batched.shape {}'.format(y1.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo sería la loss ahora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss(blank=27)\n",
    "\n",
    "output = model1(x1)  # (batch, time, n_class)\n",
    "output = torch.nn.functional.log_softmax(output, dim=2)\n",
    "output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "print(output.shape)\n",
    "print(lab1.shape)\n",
    "\n",
    "loss1 = criterion(output, lab1, x1_length,lab1_length)\n",
    "\n",
    "print('Loss1 {}'.format(loss1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment=\"_DS2V1CTC_BATCH_{batch_size}_NCNN_{n_cnn_layers}_NRNN_{n_rnn_layers}\".format(**hparams))\n",
    "# writer.add_graph(model1, x1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodificacor y función de costo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GreedyDecoder(output, labels, label_lengths, blank_label=27, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    # print(arg_maxes)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(texttransform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        if( blank_label in decode):\n",
    "            decodes.append('\\u03B5')\n",
    "        else:\n",
    "            decodes.append(texttransform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, t1 = GreedyDecoder(y1, lab1, lab1_length, blank_label=27, collapse_repeated=True)\n",
    "print(\"Decodificó: {}\\nTarget real: {}\".format(d1,t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer \n",
    "\n",
    "def cer(pred,ref):\n",
    "    return(jiwer.wer(ref, pred))\n",
    "    # return(F.edit_distance(pred, ref)/len(ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, writer):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    train_loss = 0\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(torch.int64).to(device)\n",
    "     \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        # output = output.transpose(1,2).contiguous() # (batch, time, n_class), Esto correspondía a la red anterior\n",
    "        output = torch.nn.functional.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1).contiguous() # (time, batch, n_class)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        # experiment.log_metric('loss', loss.item(), step=iter_meter.get())\n",
    "        # experiment.log_metric('learning_rate', scheduler.get_lr(), step=iter_meter.get())\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if batch_idx % 50 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        train_loss += loss.item() / len(train_loader)\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, writer):\n",
    "    print('\\nevaluating…')\n",
    "    model.eval()\n",
    "    data_test_len = len(test_loader.dataset)\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for I, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(torch.int64).to(device)\n",
    "     \n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            # output = output.transpose(1,2).contiguous() # (batch, time, n_class)\n",
    "            output = torch.nn.functional.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "            output = output.transpose(0, 1) # (batch, time, n_class)\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output, labels, label_lengths)\n",
    "            if I % 50 == 0 or I == data_test_len:\n",
    "                print('Test Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, I * len(spectrograms), data_test_len,\n",
    "                    100. * I / len(test_loader), loss.item()))\n",
    "\n",
    "\n",
    "            for j,(pred,targ) in enumerate(zip(decoded_preds,decoded_targets)):\n",
    "                # print(\"CER sentence {}: {}\".format(j, cer(ref=targ, hypo=pred)*100))\n",
    "                # print('target: {}\\nprediction: {}'.format(targ,pred))\n",
    "                test_cer.append(cer(ref=targ, pred=pred))\n",
    "        \n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f}\\n'.format(test_loss, avg_cer))\n",
    "    writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "    writer.add_scalar('CER/test', avg_cer, epoch)\n",
    "    \n",
    "    return avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(7)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# device = torch.device(\"cuda\")\n",
    "print('Device: {}'.format(device))\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "learning_rate=1e-5\n",
    "batch_size=10\n",
    "epochs=500\n",
    "hparams = {\n",
    "    \"n_cnn_layers\": 2,\n",
    "    \"n_rnn_layers\": 1,\n",
    "    \"rnn_dim\": 512,\n",
    "    \"n_class\": 28,\n",
    "    \"n_feats\": 128,\n",
    "    \"stride\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                            batch_size=hparams['batch_size'],\n",
    "                            shuffle=True,\n",
    "                            collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                            **kwargs)\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                            batch_size=hparams['batch_size'],\n",
    "                            shuffle=False,\n",
    "                            collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                            **kwargs)\n",
    "\n",
    "model1 = SpeechRecognitionModel(\n",
    "    hparams['n_cnn_layers'], \n",
    "    hparams['n_rnn_layers'], \n",
    "    hparams['rnn_dim'],\n",
    "    hparams['n_class'], \n",
    "    hparams['n_feats'], \n",
    "    hparams['stride'], \n",
    "    hparams['dropout']\n",
    "    ).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.AdamW(model1.parameters(), hparams['learning_rate'])\n",
    "criterion = nn.CTCLoss(blank=27).to(device)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                        steps_per_epoch=int(len(train_loader)),\n",
    "                                        epochs=hparams['epochs'],\n",
    "                                        anneal_strategy='linear')\n",
    "\n",
    "# iter_meter = IterMeter()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model1, device, train_loader, criterion, optimizer, scheduler, epoch, writer)\n",
    "    if (epoch % 10 == 0):\n",
    "        test(model1, device, test_loader, criterion, epoch, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_it = iter(test_loader)\n",
    "spectrograms, labels, input_lengths, label_lengths = next(data_test_it) \n",
    "spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "output = model1(spectrograms)\n",
    "output_norm = torch.nn.functional.log_softmax(output, dim=2)\n",
    "output_norm = output_norm.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "out = output_norm.transpose(0, 1).to('cpu').detach().numpy()\n",
    "lab = labels.to('cpu').numpy()\n",
    "print(out[0].shape,lab[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[fig,ax] = plt.subplots()\n",
    "ax.pcolormesh(np.exp(out[0][:400,:]))\n",
    "caracteres = np.arange(28)\n",
    "ctext = []\n",
    "for i in caracteres:\n",
    "    ctext.append(texttransform.index_map[i])\n",
    "caracteres = np.append(caracteres,[28])\n",
    "ctext.append('\\u03B5')\n",
    "ax.set_xticks(caracteres+0.5,labels=ctext)\n",
    "ax.set_ylabel('Frame')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
