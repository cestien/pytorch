{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80050c2",
   "metadata": {},
   "source": [
    "# Reconocimiento de fonemas usando CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd4c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import  Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad64a3",
   "metadata": {},
   "source": [
    "## Implementaci贸n del dataset y el dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97ec1d",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Me convierte los datos crudos para que puedan ser usados por `Dataloader`. Me permite implementar una funci贸n `__getitem__()` en la cual leemos los datos y devolvemos por ejemplo el wav y la transcripci贸n de cada dato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = 'data/train.json'\n",
    "test_json = 'data/test.json'\n",
    "valid_json = 'data/dev.json'\n",
    "\n",
    "class TimitDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.datos_json = json.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datos_json)\n",
    "    \n",
    "    def __getitem__(self,key):\n",
    "        wavdir = self.datos_json[key]['wav']\n",
    "        duration = self.datos_json[key]['duration']\n",
    "        phn = self.datos_json[key]['phn']\n",
    "        return wavdir, duration, phn\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # El batch es una lista de tuplas: [(dato1,label1), (dato2,label2),...]\n",
    "    sequences, labels = zip(*batch) # Esto devuelve: \n",
    "                                    # sequences = (dato1,dato2,...)\n",
    "                                    # labels = (label1,label2,...)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return padded_sequences, labels # Esta es la salida del dataloader\n",
    "   \n",
    "train_ds = TimitDataset(train_json)\n",
    "test_ds = TimitDataset(test_json)\n",
    "valid_ds = TimitDataset(valid_json)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "test_dl = DataLoader(train_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "valid_dl = DataLoader(train_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eda29",
   "metadata": {},
   "source": [
    "### Versi贸n creada por gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bea189ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme Vocabulary Size: 33\n",
      "Word Vocabulary Size: 23\n",
      "Speaker Vocabulary Size: 1\n",
      "\n",
      "--- Iterating through DataLoader with batch_size=2 ---\n",
      "\n",
      "Batch 1:\n",
      "  Sample IDs: ['mrws1_sx50', 'mrws1_sx230']\n",
      "  WAV Paths (first 2): ['/dbase/timit/test/dr5/mrws1/sx50.wav', '/dbase/timit/test/dr5/mrws1/sx230.wav']...\n",
      "  Durations:\n",
      "tensor([3.2320, 3.2064])\n",
      "  Speaker IDs:\n",
      "tensor([0, 0])\n",
      "  Phonemes (numerical, padded):\n",
      "tensor([[ 2, 16, 25, 30,  4,  7,  2, 19,  6,  9, 31,  4,  2, 16, 13,  2, 16,  4,\n",
      "          5,  9, 10,  4,  2, 16, 20,  2, 12, 25,  2, 16,  7,  2,  5,  4,  2,  8,\n",
      "         21, 29,  2,  3, 20,  2, 32,  9,  6,  2],\n",
      "        [ 2, 20, 21, 22, 21, 13, 14, 11, 23, 13, 24,  2, 12, 20,  6, 25, 26,  2,\n",
      "          5, 21, 27, 28,  9, 21, 29,  6, 24, 28,  2,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "  Phoneme Lengths (original):\n",
      "tensor([46, 29])\n",
      "  Words (numerical, padded):\n",
      "tensor([[18, 19, 20, 21,  2, 22,  0],\n",
      "        [11, 12, 13, 14, 15, 16, 17]])\n",
      "  Word Lengths (original):\n",
      "tensor([6, 7])\n",
      "  Phoneme Ends (padded):\n",
      "tensor([[ 2040.,  3080.,  4428.,  4720.,  5880.,  7480.,  7880.,  8600.,  9293.,\n",
      "         10680., 12040., 13000., 13953., 14680., 15640., 16680., 17348., 18200.,\n",
      "         18966., 20762., 21840., 22520., 23800., 25080., 26318., 28880., 29240.,\n",
      "         31160., 32279., 32584., 33520., 34360., 34920., 35926., 37000., 37472.,\n",
      "         38200., 39644., 42234., 42760., 43240., 44840., 45720., 48333., 49640.,\n",
      "         51680.],\n",
      "        [ 3000.,  3592.,  4600.,  8605., 10402., 12360., 13618., 16280., 17169.,\n",
      "         18757., 20469., 23000., 23443., 24520., 26402., 28600., 30160., 30600.,\n",
      "         31400., 32360., 35800., 36360., 38809., 40040., 41650., 42945., 46120.,\n",
      "         48600., 51280.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "             0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
      "             0.]])\n",
      "  Phoneme Ends Lengths (original):\n",
      "tensor([46, 29])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "\n",
    "class SpeechJsonDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for reading speech data from a JSON file.\n",
    "\n",
    "    The JSON file is expected to have a top-level dictionary where keys are\n",
    "    sample IDs and values are dictionaries containing speech data attributes\n",
    "    like 'wav', 'duration', 'spk_id', 'phn', 'wrd', and 'ground_truth_phn_ends'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, json_file_path):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by loading the JSON file and building vocabularies.\n",
    "\n",
    "        Args:\n",
    "            json_file_path (str): The path to the JSON dataset file.\n",
    "        \"\"\"\n",
    "        # Load the JSON data\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        # Get a list of all sample IDs (keys in the top-level dictionary)\n",
    "        self.sample_ids = list(self.data.keys())\n",
    "\n",
    "        # Build vocabularies for phonemes, words, and speaker IDs\n",
    "        self.phn_vocab = {\"<PAD>\": 0, \"<UNK>\": 1} # Start with PAD and UNK tokens\n",
    "        self.wrd_vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        self.spk_vocab = {}\n",
    "\n",
    "        self._build_vocabularies()\n",
    "\n",
    "    def _build_vocabularies(self):\n",
    "        \"\"\"\n",
    "        Iterates through the data to build numerical vocabularies for phonemes,\n",
    "        words, and speaker IDs.\n",
    "        \"\"\"\n",
    "        phn_idx_counter = 2 # Start from 2 as 0 and 1 are reserved for PAD/UNK\n",
    "        wrd_idx_counter = 2\n",
    "        spk_idx_counter = 0\n",
    "\n",
    "        for sample_id in self.sample_ids:\n",
    "            entry = self.data[sample_id]\n",
    "\n",
    "            # Process phonemes\n",
    "            phonemes = entry.get('phn', '').split()\n",
    "            for phn in phonemes:\n",
    "                if phn not in self.phn_vocab:\n",
    "                    self.phn_vocab[phn] = phn_idx_counter\n",
    "                    phn_idx_counter += 1\n",
    "\n",
    "            # Process words\n",
    "            words = entry.get('wrd', '').split()\n",
    "            for wrd in words:\n",
    "                if wrd not in self.wrd_vocab:\n",
    "                    self.wrd_vocab[wrd] = wrd_idx_counter\n",
    "                    wrd_idx_counter += 1\n",
    "\n",
    "            # Process speaker ID\n",
    "            spk_id = entry.get('spk_id')\n",
    "            if spk_id and spk_id not in self.spk_vocab:\n",
    "                self.spk_vocab[spk_id] = spk_idx_counter\n",
    "                spk_idx_counter += 1\n",
    "\n",
    "        print(f\"Phoneme Vocabulary Size: {len(self.phn_vocab)}\")\n",
    "        print(f\"Word Vocabulary Size: {len(self.wrd_vocab)}\")\n",
    "        print(f\"Speaker Vocabulary Size: {len(self.spk_vocab)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a single sample from the dataset at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the processed data for the sample.\n",
    "                  Keys include 'sample_id', 'wav_path', 'duration',\n",
    "                  'speaker_id', 'phonemes', 'words', 'phoneme_ends'.\n",
    "        \"\"\"\n",
    "        sample_id = self.sample_ids[idx]\n",
    "        entry = self.data[sample_id]\n",
    "\n",
    "        # Extract raw data\n",
    "        wav_path = entry.get('wav', '')\n",
    "        duration = entry.get('duration', 0.0)\n",
    "\n",
    "        # Convert speaker ID to numerical\n",
    "        spk_id_str = entry.get('spk_id', '')\n",
    "        speaker_id = self.spk_vocab.get(spk_id_str, -1) # -1 for unknown speaker\n",
    "\n",
    "        # Process phonemes: tokenize and numericalize\n",
    "        phonemes_raw = entry.get('phn', '').split()\n",
    "        phonemes_numerical = [self.phn_vocab.get(p, self.phn_vocab[\"<UNK>\"]) for p in phonemes_raw]\n",
    "        # Convert to tensor; padding will be handled by collate_fn\n",
    "        phonemes_tensor = torch.tensor(phonemes_numerical, dtype=torch.long)\n",
    "\n",
    "        # Process words: tokenize and numericalize\n",
    "        words_raw = entry.get('wrd', '').split()\n",
    "        words_numerical = [self.wrd_vocab.get(w, self.wrd_vocab[\"<UNK>\"]) for w in words_raw]\n",
    "        # Convert to tensor; padding will be handled by collate_fn\n",
    "        words_tensor = torch.tensor(words_numerical, dtype=torch.long)\n",
    "\n",
    "        # Process ground_truth_phn_ends: convert to list of floats and then to tensor\n",
    "        phn_ends_raw = entry.get('ground_truth_phn_ends', '').split()\n",
    "        phoneme_ends = [float(end) for end in phn_ends_raw if end.strip()] # Ensure no empty strings\n",
    "        phoneme_ends_tensor = torch.tensor(phoneme_ends, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            'sample_id': sample_id,\n",
    "            'wav_path': wav_path,\n",
    "            'duration': torch.tensor(duration, dtype=torch.float32),\n",
    "            'speaker_id': torch.tensor(speaker_id, dtype=torch.long),\n",
    "            'phonemes': phonemes_tensor,\n",
    "            'words': words_tensor,\n",
    "            'phoneme_ends': phoneme_ends_tensor\n",
    "        }\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences (phonemes, words, phoneme_ends)\n",
    "    by padding them to the maximum length within each batch.\n",
    "\n",
    "    Args:\n",
    "        batch (list): A list of dictionaries, where each dictionary is a sample\n",
    "                      returned by the __getitem__ method of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of batched tensors and other data.\n",
    "    \"\"\"\n",
    "    # Find max lengths in the current batch for padding\n",
    "    max_phn_len = max(len(item['phonemes']) for item in batch)\n",
    "    max_wrd_len = max(len(item['words']) for item in batch)\n",
    "    max_phn_ends_len = max(len(item['phoneme_ends']) for item in batch)\n",
    "\n",
    "    padded_phonemes = []\n",
    "    padded_words = []\n",
    "    padded_phoneme_ends = []\n",
    "    sample_ids = []\n",
    "    wav_paths = []\n",
    "    durations = []\n",
    "    speaker_ids = []\n",
    "\n",
    "    for item in batch:\n",
    "        # Pad phonemes\n",
    "        phn_len = len(item['phonemes'])\n",
    "        padded_phn = torch.cat([\n",
    "            item['phonemes'],\n",
    "            torch.tensor([0] * (max_phn_len - phn_len), dtype=torch.long) # 0 is PAD_ID\n",
    "        ])\n",
    "        padded_phonemes.append(padded_phn)\n",
    "\n",
    "        # Pad words\n",
    "        wrd_len = len(item['words'])\n",
    "        padded_wrd = torch.cat([\n",
    "            item['words'],\n",
    "            torch.tensor([0] * (max_wrd_len - wrd_len), dtype=torch.long) # 0 is PAD_ID\n",
    "        ])\n",
    "        padded_words.append(padded_wrd)\n",
    "\n",
    "        # Pad phoneme ends\n",
    "        phn_ends_len = len(item['phoneme_ends'])\n",
    "        padded_pe = torch.cat([\n",
    "            item['phoneme_ends'],\n",
    "            torch.tensor([0.0] * (max_phn_ends_len - phn_ends_len), dtype=torch.float32) # 0.0 is PAD_ID\n",
    "        ])\n",
    "        padded_phoneme_ends.append(padded_pe)\n",
    "\n",
    "        # Collect other data\n",
    "        sample_ids.append(item['sample_id'])\n",
    "        wav_paths.append(item['wav_path'])\n",
    "        durations.append(item['duration'])\n",
    "        speaker_ids.append(item['speaker_id'])\n",
    "\n",
    "    return {\n",
    "        'sample_id': sample_ids, # List of strings, not a tensor\n",
    "        'wav_path': wav_paths,   # List of strings, not a tensor\n",
    "        'duration': torch.stack(durations),\n",
    "        'speaker_id': torch.stack(speaker_ids),\n",
    "        'phonemes': torch.stack(padded_phonemes),\n",
    "        'words': torch.stack(padded_words),\n",
    "        'phoneme_ends': torch.stack(padded_phoneme_ends),\n",
    "        'phoneme_lengths': torch.tensor([len(item['phonemes']) for item in batch], dtype=torch.long), # Store original lengths\n",
    "        'word_lengths': torch.tensor([len(item['words']) for item in batch], dtype=torch.long),\n",
    "        'phoneme_ends_lengths': torch.tensor([len(item['phoneme_ends']) for item in batch], dtype=torch.long)\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create a dummy p.json file for demonstration if it doesn't exist\n",
    "    # In a real scenario, you would already have this file.\n",
    "    try:\n",
    "        with open('p.json', 'x') as f:\n",
    "            f.write(\"\"\"\n",
    "{\n",
    "  \"mrws1_sx320\": {\n",
    "    \"wav\": \"/dbase/timit/test/dr5/mrws1/sx320.wav\",\n",
    "    \"duration\": 3.28325,\n",
    "    \"spk_id\": \"mrws1\",\n",
    "    \"phn\": \"sil dh ih n ih r ih s ih n ih sil g aa sil m ey n aa sil b iy w ih th ih n w aa sil k ih ng sil d ih s sil t ih n sil s sil\",\n",
    "    \"wrd\": \"the nearest synagogue may not be within walking distance\",\n",
    "    \"ground_truth_phn_ends\": \"2360 2840 3216 4511 5556 7018 7880 10440 11160 12040 13160 13960 14200 17640 18280 19160 20360 21560 23800 25320 25720 26520 27800 28825 30440 31248 32208 34130 35880 36760 37640 37960 39101 40120 40360 41640 43000 43320 44200 44440 45280 46680 49560 52480\"\n",
    "  },\n",
    "  \"mrws1_sx230\": {\n",
    "    \"wav\": \"/dbase/timit/test/dr5/mrws1/sx230.wav\",\n",
    "    \"duration\": 3.2064375,\n",
    "    \"spk_id\": \"mrws1\",\n",
    "    \"phn\": \"sil ah l aw l iy w ey hh iy er sil b ah r ae sh sil n l ay z aa l eh r er z sil\",\n",
    "    \"wrd\": \"allow leeway here but rationalize all errors\",\n",
    "    \"ground_truth_phn_ends\": \"3000 3592 4600 8605 10402 12360 13618 16280 17169 18757 20469 23000 23443 24520 26402 28600 30160 30600 31400 32360 35800 36360 38809 40040 41650 42945 46120 48600 51280\"\n",
    "  },\n",
    "  \"mrws1_sx50\": {\n",
    "    \"wav\": \"/dbase/timit/test/dr5/mrws1/sx50.wav\",\n",
    "    \"duration\": 3.232,\n",
    "    \"spk_id\": \"mrws1\",\n",
    "    \"phn\": \"sil k ae dx ih s sil t r aa f ih sil k iy sil k ih n aa m ih sil k ah sil b ae sil k s sil n ih sil g l eh sil dh ah sil p aa r sil\",\n",
    "    \"wrd\": \"catastrophic economic cutbacks neglect the poor\",\n",
    "    \"ground_truth_phn_ends\": \"2040 3080 4428 4720 5880 7480 7880 8600 9293 10680 12040 13000 13953 14680 15640 16680 17348 18200 18966 20762 21840 22520 23800 25080 26318 28880 29240 31160 32279 32584 33520 34360 34920 35926 37000 37472 38200 39644 42234 42760 43240 44840 45720 48333 49640 51680\"\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "    except FileExistsError:\n",
    "        print(\"p.json already exists. Skipping dummy file creation.\")\n",
    "\n",
    "    json_file_path = 'p.json'\n",
    "\n",
    "    # 1. Create the dataset instance\n",
    "    dataset = SpeechJsonDataset(json_file_path)\n",
    "\n",
    "    # 2. Create a DataLoader instance\n",
    "    # Set batch_size and shuffle as needed for training\n",
    "    batch_size = 2\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "    # 3. Iterate through the DataLoader to see the batched output\n",
    "    print(f\"\\n--- Iterating through DataLoader with batch_size={batch_size} ---\")\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print(f\"\\nBatch {i+1}:\")\n",
    "        print(f\"  Sample IDs: {batch['sample_id']}\")\n",
    "        print(f\"  WAV Paths (first 2): {batch['wav_path'][:2]}...\")\n",
    "        print(f\"  Durations:\\n{batch['duration']}\")\n",
    "        print(f\"  Speaker IDs:\\n{batch['speaker_id']}\")\n",
    "        print(f\"  Phonemes (numerical, padded):\\n{batch['phonemes']}\")\n",
    "        print(f\"  Phoneme Lengths (original):\\n{batch['phoneme_lengths']}\")\n",
    "        print(f\"  Words (numerical, padded):\\n{batch['words']}\")\n",
    "        print(f\"  Word Lengths (original):\\n{batch['word_lengths']}\")\n",
    "        print(f\"  Phoneme Ends (padded):\\n{batch['phoneme_ends']}\")\n",
    "        print(f\"  Phoneme Ends Lengths (original):\\n{batch['phoneme_ends_lengths']}\")\n",
    "\n",
    "        # Example of how you might use these in a model:\n",
    "        # model_output = your_model(batch['phonemes'], batch['phoneme_lengths'])\n",
    "        # loss = criterion(model_output, batch['speaker_id'])\n",
    "\n",
    "        if i == 0: # Only print the first batch for brevity\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
