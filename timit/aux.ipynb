{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8f926fa",
   "metadata": {},
   "source": [
    "# Notebook auxiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae35e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchaudio\n",
    "from matplotlib import pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824a532",
   "metadata": {},
   "source": [
    "#### La función `collate_fn` (para construir minibatches con datos de diferente longitud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a4fcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 9, 8, 8]) tensor(1)\n",
      "tensor([5, 1, 3, 3, 8, 2, 3, 1, 0, 0]) tensor(1)\n",
      "tensor([6, 0, 5, 8, 8, 6, 2, 8]) tensor(1)\n",
      "tensor([5, 6, 1, 3, 8, 6]) tensor(1)\n",
      "tensor([7, 8, 3, 3, 4, 9, 6, 7, 2, 8, 7, 1]) tensor(0)\n",
      "\n",
      "Batch 1:\n",
      "Padded Sequences:\n",
      "tensor([[2, 0, 9, 8, 8, 0, 0, 0, 0, 0],\n",
      "        [5, 1, 3, 3, 8, 2, 3, 1, 0, 0]])\n",
      "Labels:\n",
      "tensor([1, 1])\n",
      "Shape of padded sequences:\n",
      "torch.Size([2, 10])\n",
      "Batch 2:\n",
      "Padded Sequences:\n",
      "tensor([[6, 0, 5, 8, 8, 6, 2, 8],\n",
      "        [5, 6, 1, 3, 8, 6, 0, 0]])\n",
      "Labels:\n",
      "tensor([1, 1])\n",
      "Shape of padded sequences:\n",
      "torch.Size([2, 8])\n",
      "Batch 3:\n",
      "Padded Sequences:\n",
      "tensor([[7, 8, 3, 3, 4, 9, 6, 7, 2, 8, 7, 1]])\n",
      "Labels:\n",
      "tensor([0])\n",
      "Shape of padded sequences:\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VariableLengthDataset(Dataset):\n",
    "    def __init__(self): # Inventamos un dataset de longitud variable\n",
    "        self.data = [\n",
    "            torch.randint(0, 10, (length,)) for length in [5, 10, 8, 6, 12]\n",
    "        ]\n",
    "        self.labels = torch.randint(0, 2, (len(self.data),))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # El batch es una lista de tuplas: [(dato1,label1), (dato2,label2),...]\n",
    "    sequences, labels = zip(*batch) # Esto devuelve: \n",
    "                                    # sequences = (dato1,dato2,...)\n",
    "                                    # labels = (label1,label2,...)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return padded_sequences, labels # Esta es la salida del dataloader\n",
    "\n",
    "dataset = VariableLengthDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dato,label =  dataset[i]\n",
    "    print(dato,label)\n",
    "print()\n",
    "\n",
    "for batch_idx, (padded_sequences, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Padded Sequences:\")\n",
    "    print(padded_sequences)\n",
    "    print(\"Labels:\")\n",
    "    print(labels)\n",
    "    print(\"Shape of padded sequences:\")\n",
    "    print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0f629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 9, 8, 8]) tensor(1)\n",
      "tensor([5, 1, 3, 3, 8, 2, 3, 1, 0, 0]) tensor(1)\n",
      "tensor([6, 0, 5, 8, 8, 6, 2, 8]) tensor(1)\n",
      "tensor([5, 6, 1, 3, 8, 6]) tensor(1)\n",
      "tensor([7, 8, 3, 3, 4, 9, 6, 7, 2, 8, 7, 1]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dato,label =  dataset[i]\n",
    "    print(dato,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4980f4",
   "metadata": {},
   "source": [
    "##### Sobre `zip` y el operador `*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bb68f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a b c\n",
      "a b c\n",
      "('x', 'y', 'z')\n",
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Si pasamos una lista como argumento a una función y le aplicamos el operador * a dicha lista,\n",
    "# el operador \"desarma\" la lista y la convierte en argumentos separados para la función\n",
    "x = ['a','b', 'c']\n",
    "print(*x)\n",
    "print(x[0],x[1],x[2])\n",
    "\n",
    "# zip es un iterador de tuplas:\n",
    "# tuplas de entrada: [(a,1),(b,2),(c,3)]\n",
    "# tuplas de salida: ([a,b,c),(1,2,3)]\n",
    "l = [('x',1),('y',2),('z',3)]\n",
    "ll = zip(*l)\n",
    "for i in ll:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdf0959",
   "metadata": {},
   "source": [
    "##### Sobre tteradores e iterables\n",
    "  - Un iterable es cualquier objeto que se pueda recorrer o *iterar*, por ejemplo una lista, una tupla, un rango, un diccionario. \n",
    "  - Un iterador es un objeto que implementa el protocolo de *iterador*, es decir, un objeto que tiene definidos los métodos:\n",
    "    - `__iter__` que devuelve el propio objeto iterador\n",
    "    - `__next__` que devuelve el siguiente elemento de la secuencia y un `StopIteration` cuando la secuencia llegó a su fin\n",
    "  - La estructura de control `for`:\n",
    "    - recibe un iterable, por ejemplo `range(10)`\n",
    "    - le aplica `iter()` con lo cual lo convierte en iterador\n",
    "    - llama repetidamente a `next()` hasta que aparezca la excepción `StopIteration` y termina\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8ebf199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [1,2,3,4,5] # L es un iterable\n",
    "iter_L = iter(L) # iter_L es un iterador\n",
    "next(iter_L) # Devuelve el primer elemento del iterador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d527e3",
   "metadata": {},
   "source": [
    "##### Sobre los generadores\n",
    "Un generador es un objeto de tipo iterador creado por:\n",
    "  - *Una función generadora*. Es decir, una función que devuelve sus resultados usando `yield` en lugar de `return`. Esto hace que la función recuerde el estado cuando se la deja y lo retome cuando se la llame de nuevo. \n",
    "  - *Una expresión generadora*. Es igual que la comprehension list solo que se usan paréntesis en lugar de corchetes.\n",
    "\n",
    "Ojo, no todo objeto de tipo iterador es un objeto generador, solo los que son creados por una función generadora.\n",
    "\n",
    "También se podría crear un generador como instancia de una clase siempre que dicha tenga implementado los métodos `__iter__` y `__next__` para convertirla en iterador y los elementos del objeto sean creados con un método generador, es decir una función de que devuelva mediante `yield`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d62969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "0 2\n"
     ]
    }
   ],
   "source": [
    "# Función generadora\n",
    "def even_numbers(limit):\n",
    "    num = 0\n",
    "    while num < limit:\n",
    "        yield num\n",
    "        num += 2\n",
    "gen_even = even_numbers(10) # gen_even es un generador\n",
    "for n in even_numbers(10):\n",
    "    print(n)\n",
    "\n",
    "print(next(gen_even),next(gen_even)) # Devuelve el siguiente elemento del generador\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76d42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Expresión generadora\n",
    "gen_even = (num for num in range(10) if num % 2 == 0) \n",
    "for n in gen_even:\n",
    "    print(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96740af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "<class '__main__.NumerosCuadrados'>\n"
     ]
    }
   ],
   "source": [
    "# Clase que se comporta como un iterador\n",
    "class NumerosCuadrados:\n",
    "    def __init__(self, limite):\n",
    "        self.limite = limite\n",
    "        self.actual = 0 # Maneja el estado\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self # Un iterador es un iterable que se devuelve a sí mismo\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.actual < self.limite:\n",
    "            valor = self.actual * self.actual\n",
    "            self.actual += 1 # Actualiza el estado para la próxima llamada\n",
    "            return valor\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "# Uso de la clase iterador\n",
    "cuadrados_obj = NumerosCuadrados(5)\n",
    "\n",
    "for num in cuadrados_obj:\n",
    "    print(num)\n",
    "\n",
    "print(type(cuadrados_obj)) # Salida: <class '__main__.NumerosCuadrados'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c1965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Clase que contiene un método generador\n",
    "# Para iterar con una instancia de esta \n",
    "# clase no es necesario implementar los métodos __iter__ y __next__\n",
    "# porque el método generador ya lo hace implícitamente ya que todo generador es un iterador.\n",
    "class SecuenciaPersonalizada:\n",
    "    def __init__(self, inicio, fin):\n",
    "        self.inicio = inicio\n",
    "        self.fin = fin\n",
    "\n",
    "    def generar_rango(self): # Esto es un método generador\n",
    "        actual = self.inicio\n",
    "        while actual <= self.fin:\n",
    "            yield actual\n",
    "            actual += 1\n",
    "\n",
    "# Uso de la clase con un método generador\n",
    "mi_secuencia = SecuenciaPersonalizada(1, 5)\n",
    "\n",
    "# Llamar al método generador para obtener un objeto generador\n",
    "gen_obj = mi_secuencia.generar_rango()\n",
    "\n",
    "print(type(gen_obj)) # Salida: <class 'generator'>\n",
    "\n",
    "for num in gen_obj:\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d9b384",
   "metadata": {},
   "source": [
    "##### Diferencia entre generadores y clases/instancias\n",
    "  - Clases/Instancias: Mantienen el estado de los datos (variables de instancia) a través de múltiples llamadas a sus métodos. Los métodos, a menos que usen yield, se ejecutan de forma completa cada vez.\n",
    "  - Generadores: Mantienen el estado de la ejecución (variables locales, punto de pausa) y permiten \"pausar\" y \"reanudar\" la ejecución de la función.\n",
    "\n",
    "Puedes tener métodos de clase que sean generadores (si contienen yield), pero esto es una elección de diseño específica y no una propiedad inherente de todos los métodos de clase.\n",
    "\n",
    "Recordar que `__iter__` siempre debe devolver un objeto de tipo iterador. En el siguiente ejemplo esto lo hacemos devolviendo con `yield` ya que de este modo se convierte a `__iter__` en una función generadora y por lo tanto devolverá un iterador (de tipo generador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f83fdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-2\n",
      "-3\n",
      "-4\n",
      "-5\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "class Prueba():\n",
    "    def __init__(self, x):\n",
    "        self.x = a\n",
    "\n",
    "    def __iter__(self):\n",
    "    #    return self.x\n",
    "        for i in self.x:\n",
    "            yield  -i\n",
    "p = Prueba(a)\n",
    "\n",
    "for i in p:\n",
    "    print(i)  # Salida: 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe183d",
   "metadata": {},
   "source": [
    "##### La función `stack` (unión de tensores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "  \n",
    "# creating tensors \n",
    "x = torch.tensor([1.,3.,6.,10.]) \n",
    "y = torch.tensor([2.,7.,9.,13.]) \n",
    "# printing above created tensors \n",
    "print(\"Tensor x:\", x) \n",
    "print(\"Tensor y:\", y) \n",
    "  \n",
    "# join above tensor using \"torch.stack()\" \n",
    "print(\"join tensors:\") \n",
    "t = torch.stack((x,y)) \n",
    "  \n",
    "# print final tensor after join \n",
    "print(t) \n",
    "\n",
    "\n",
    "print(\"join tensors dimension 0:\") \n",
    "t = torch.stack((x,y), dim = 0) \n",
    "print(t) \n",
    "  \n",
    "print(\"join tensors dimension 1:\") \n",
    "t = torch.stack((x,y), dim = 1) \n",
    "print(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aca08a",
   "metadata": {},
   "source": [
    "#### Reproducibilidad de los experimentos\n",
    "Como fijar la semilla y aplicarla a todas las posibles variaciones aleatorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2713b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random # También para operaciones random de Python\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed) # Para todas las GPUs\n",
    "    torch.cuda.manual_seed_all(seed) # Para múltiples GPUs\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Algunas operaciones de cuDNN pueden ser no deterministas, se recomienda esto:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # Puede hacer que el entrenamiento sea más lento\n",
    "\n",
    "# Definir la semilla que usaremos\n",
    "MY_SEED = 42\n",
    "set_seed(MY_SEED)\n",
    "\n",
    "# Acá empieza el código de la aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9086f9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import jiwer \n",
    "# Pruebitas para definir 'cer'\n",
    "reference = 'chau'\n",
    "cer_score = F.edit_distance('ciao', reference)/len(reference)\n",
    "print(cer_score)\n",
    "jcer_score = jiwer.cer(reference, 'ciao')\n",
    "print(jcer_score)\n",
    "\n",
    "# Pero en realidad en este programa uso el wer, porque el GreedyDecoder nos da las salidas en caracteres separados por blancos\n",
    "# (como si fueran palabras). Y jiwer tiene los dos, así que usamos el wer del jiwer. \n",
    "# Después sirve como excusa para ver los alineamientos. \n",
    "def cer(pred,ref):\n",
    "    return(jiwer.wer(ref, pred))\n",
    "    # return(F.edit_distance(pred, ref)/len(ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427f530",
   "metadata": {},
   "source": [
    "##### Sobre `model.train()`, `model.eval()` y `with torch.no_grad()`\n",
    "  - Usar `model.train()` cuando entrenamos para que `BatchNormalization` y `dropout` funcionen correctamente\n",
    "  - Usar `model.eval()` cuando hacemos test o validación.\n",
    "  - Usar `with torch.no_grad()` es decir no calcular el gradiente dentro de lo que esté en el bloque `with`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac26cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training mode\n",
    "model.train()\n",
    "# Your training loop\n",
    "# ...\n",
    "# Now switch to evaluation mode for validation\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():  # No gradient calculation for evaluation\n",
    "    out_data = model(data)\n",
    "\n",
    "# Don't forget to switch back to training mode!\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af162e",
   "metadata": {},
   "source": [
    "\n",
    "##### Esquema general\n",
    "  - De alguna manera generamos el dataset que consta de (tensores):\n",
    "      - (x_train, y_train)\n",
    "      - (x_valid, y_valid)\n",
    "      - (x_test, y_test)\n",
    "  - Lo convertimos en un dataset que es un wrap que permite iterar sobre los datos. Podemos hacerlo nosotros o podemos usar `TensorDataset`\n",
    "  - Convertimos el dataset en un dataloader que es una versión del dataset separada en batches. Esto lo hacemos con `DataLoader`. Si es necesario, a `DataLoader` les podemos pasar una función `collate_fn` que por ejemplo haga un padding si los datos no son todos de la misma longitud. También puede hacer un shuffle de los datos en cada epoch, lo cual es bueno en el entrenamiento. El bs de validación se puede hacer más grande ya que no necesita calcular gradientes ni hacer back propagation.\n",
    "\n",
    "  Las siguientes sentencias:\n",
    "  \n",
    "    - `loss_fn = nn.CrossEntropyLoss()`\n",
    "    - `loss_func = torch.nn.functional.cross_entropy`\n",
    "\n",
    "  Hacen lo mismo, pero loss_fn es un objeto y loss_func es una función. Es un tema de programación estructurada, las dos hacen lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688fd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "test_ds = TensorDataset(x_test, y_test)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs*2)\n",
    "test_dl = DataLoader(test_ds, batch_size=bs)\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    opt = optim.SGD(model.parameters(), lr=lr) \n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step() #for p in model.parameters(): p -= p.grad * lr\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    opt = optim.SGD(model.parameters(), lr=lr) \n",
    "    test_loss, correct = 0, 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl: # Batch de train\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step() #for p in model.parameters(): p -= p.grad * lr\n",
    "            opt.zero_grad()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                test_loss += loss_func(pred,yb).item()\n",
    "\n",
    "            correct += (pred.argmax(1) == yb).type(torch.float).sum().item()\n",
    "\n",
    "            losses, nums = zip(\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
    "            )\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef17bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([0.1,0.7,0.3,0,0.9])\n",
    "a.argmax().type(float32) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af139cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "au = {'key': 'mbwm0_si1934', 'scored': True, 'hyp_absent': False, 'hyp_empty': False, 'num_edits': 7, \n",
    " 'num_ref_tokens': 13, 'WER': 53.84615384615385, 'insertions': 0, 'deletions': 7, 'substitutions': 0, \n",
    " 'alignment': [(...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...), (...)], \n",
    " 'ref_tokens': [['sil', 'w', 'ey', 'dx', 'ah', 'l', 'ih', 'dx', 'l', 'w', 'ay', 'l', 'sil']], \n",
    " 'hyp_tokens': [['sil', 'w', 'ey', 'w', 'l', 'sil']]}\n",
    "summary = {'WER': 55.38594854019464, 'SER': 100.0, 'num_edits': 8309, 'num_scored_tokens': 15002, \n",
    "           'num_erroneous_sents': 400, 'num_scored_sents': 400, 'num_absent_sents': 0, \n",
    "           'num_ref_sents': 400, 'insertions': 191, 'deletions': 5415, \n",
    "           'substitutions': 2703, 'error_rate': 55.38594854019464}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb8e101b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WER': 53.84615384615385,\n",
       " 'SER': 100.0,\n",
       " 'num_edits': 7,\n",
       " 'num_scored_tokens': 13,\n",
       " 'num_erroneous_sents': 1,\n",
       " 'num_scored_sents': 1,\n",
       " 'num_absent_sents': 0,\n",
       " 'num_ref_sents': 1,\n",
       " 'insertions': 0,\n",
       " 'deletions': 7,\n",
       " 'substitutions': 0,\n",
       " 'error_rate': 53.84615384615385}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import speechbrain\n",
    "cer_stats = speechbrain.utils.metric_stats.ErrorRateStats()\n",
    "cer_stats.append(ids=['au'], predict = au['hyp_tokens'], target = au['ref_tokens'])\n",
    "cer_stats.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72e144c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p.json already exists. Skipping dummy file creation.\n"
     ]
    }
   ],
   "source": [
    "# Creación  de un archivo JSON\n",
    "\n",
    "try:\n",
    "    with open('p.json', 'x') as f:\n",
    "        f.write(\"\"\"\n",
    "{\n",
    "\"mrws1_sx320\": {\n",
    "\"wav\": \"/dbase/timit/test/dr5/mrws1/sx320.wav\",\n",
    "\"duration\": 3.28325,\n",
    "\"spk_id\": \"mrws1\",\n",
    "\"phn\": \"sil dh ih n ih r ih s ih n ih sil g aa sil m ey n aa sil b iy w ih th ih n w aa sil k ih ng sil d ih s sil t ih n sil s sil\",\n",
    "\"wrd\": \"the nearest synagogue may not be within walking distance\",\n",
    "\"ground_truth_phn_ends\": \"2360 2840 3216 4511 5556 7018 7880 10440 11160 12040 13160 13960 14200 17640 18280 19160 20360 21560 23800 25320 25720 26520 27800 28825 30440 31248 32208 34130 35880 36760 37640 37960 39101 40120 40360 41640 43000 43320 44200 44440 45280 46680 49560 52480\"\n",
    "},\n",
    "} \"\"\")\n",
    "except FileExistsError:\n",
    "    print(\"p.json already exists. Skipping dummy file creation.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99be6b",
   "metadata": {},
   "source": [
    "## Creación de un greedy ctc decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def greedy_ctc_decode(emissions: torch.Tensor, blank_idx: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Performs greedy CTC decoding on a batch of log-probabilities.\n",
    "\n",
    "    Args:\n",
    "        emissions: Tensor of shape (seq_len, batch_size, num_classes)\n",
    "                   containing log-probabilities.\n",
    "        blank_idx: Index of the blank token.\n",
    "\n",
    "    Returns:\n",
    "        A list of decoded strings, one for each sequence in the batch.\n",
    "    \"\"\"\n",
    "    decoded_sequences = []\n",
    "    # Permute to (batch_size, seq_len, num_classes) for easier argmax\n",
    "    emissions = emissions.permute(1, 0, 2)\n",
    "\n",
    "    for i in range(emissions.shape[0]): # Iterate over batch\n",
    "        # Get the index of the max probability at each timestep\n",
    "        argmax_preds = emissions[i].argmax(dim=-1)\n",
    "\n",
    "        decoded_seq = []\n",
    "        last_char_idx = -1\n",
    "        for char_idx in argmax_preds:\n",
    "            if char_idx != blank_idx and (char_idx != last_char_idx or last_char_idx == blank_idx):\n",
    "                # Add if not blank and not a repeated character (unless the last was blank)\n",
    "                decoded_seq.append(char_idx.item())\n",
    "            last_char_idx = char_idx\n",
    "\n",
    "        # Convert indices to actual characters (you'll need your vocab mapping)\n",
    "        # For this example, let's assume `tokens` list is available\n",
    "        decoded_strings = [tokens[idx] for idx in decoded_seq]\n",
    "        decoded_sequences.append(\"\".join(decoded_strings))\n",
    "    return decoded_sequences\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"<blank>\", \"a\", \"b\", \"c\", \" \"] # Your actual vocabulary\n",
    "blank_idx = tokens.index(\"<blank>\")\n",
    "\n",
    "# Example emissions (seq_len, batch_size, num_classes)\n",
    "# Let's say the model outputs: \"a-a-b-blank-b-c\" (where '-' is blank)\n",
    "# This should decode to \"abc\"\n",
    "emissions_example = torch.zeros(7, 1, len(tokens))\n",
    "emissions_example[0, 0, tokens.index(\"a\")] = 10\n",
    "emissions_example[1, 0, blank_idx] = 10\n",
    "emissions_example[2, 0, tokens.index(\"a\")] = 10\n",
    "emissions_example[3, 0, blank_idx] = 10\n",
    "emissions_example[4, 0, tokens.index(\"b\")] = 10\n",
    "emissions_example[5, 0, blank_idx] = 10\n",
    "emissions_example[6, 0, tokens.index(\"c\")] = 10\n",
    "\n",
    "# Add a small amount of noise to other classes to avoid all zeros in softmax\n",
    "emissions_example += torch.randn_like(emissions_example) * 0.1\n",
    "\n",
    "decoded_greedy = greedy_ctc_decode(emissions_example, blank_idx)\n",
    "print(f\"Greedy decoded: {decoded_greedy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f301a60",
   "metadata": {},
   "source": [
    "## Bloque de CNN, RNN y DNN teniendo en cuenta el padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd43aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos dummy creados para la demostración.\n",
      "\n",
      "Usando dispositivo: cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cestien/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.15 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del Modelo CRDNN:\n",
      "CRDNNModel(\n",
      "  (mel_transform): MelSpectrogram(\n",
      "    (spectrogram): Spectrogram()\n",
      "    (mel_scale): MelScale()\n",
      "  )\n",
      "  (cnn_blocks): ModuleList(\n",
      "    (0): CNN_block(\n",
      "      (bloque_cnn): Sequential(\n",
      "        (0): Conv1d(80, 64, kernel_size=(3,), stride=(1,), padding=same, padding_mode=replicate)\n",
      "        (1): Transpose()\n",
      "        (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Transpose()\n",
      "        (4): LeakyReLU(negative_slope=0.01)\n",
      "        (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Dropout(p=0.15, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): CNN_block(\n",
      "      (bloque_cnn): Sequential(\n",
      "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=same, padding_mode=replicate)\n",
      "        (1): Transpose()\n",
      "        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (3): Transpose()\n",
      "        (4): LeakyReLU(negative_slope=0.01)\n",
      "        (5): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "        (6): Dropout(p=0.15, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (rnn_blocks): ModuleList(\n",
      "    (0): GRU(128, 256, batch_first=True, dropout=0.15, bidirectional=True)\n",
      "    (1): GRU(512, 256, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (dnn_blocks): ModuleList(\n",
      "    (0-2): 3 x Sequential(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Linear(in_features=512, out_features=40, bias=True)\n",
      ")\n",
      "\n",
      "--- Probando un batch a través del modelo ---\n",
      "Batch 1:\n",
      "  Input Waveforms Shape: torch.Size([2, 40000])\n",
      "  Input Waveform Lengths (Original): tensor([40000, 32000], device='cuda:0')\n",
      "  Logits Shape (salida del modelo): torch.Size([2, 246, 40])\n",
      "  Output Lengths (después de Mel, CNNs y RNNs): tensor([246, 196], device='cuda:0')\n",
      "------------------------------\n",
      "\n",
      "¡Modelo CRDNN integrado y probado con éxito!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import json\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import math\n",
    "\n",
    "# --- Utilidad: Transpose Layer ---\n",
    "# Esta clase es necesaria porque tu CNN_block la utiliza para permutar dimensiones\n",
    "# antes y después de LayerNorm, ya que LayerNorm suele aplicarse sobre la dimensión de características.\n",
    "class Transpose(nn.Module):\n",
    "    \"\"\"\n",
    "    Un módulo simple para permutar las dimensiones de un tensor.\n",
    "    Útil para insertar en nn.Sequential donde se necesita una operación de transposición.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim0: int, dim1: int):\n",
    "        super().__init__()\n",
    "        self.dim0 = dim0\n",
    "        self.dim1 = dim1\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.transpose(self.dim0, self.dim1)\n",
    "\n",
    "# --- Tu CNN_block ---\n",
    "class CNN_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloque convolucional 1D con normalización de capa y manejo de padding.\n",
    "    Diseñado para procesar secuencias de características (ej. Mel-espectrogramas).\n",
    "    El input esperado es (batch, in_channels, sequence_length).\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Número de canales de entrada (ej. n_mels para el primer bloque).\n",
    "        out_channels (int): Número de canales de salida.\n",
    "        kernel_size (int): Tamaño del kernel convolucional (para Conv1d).\n",
    "        pool_kernel_size (int): Tamaño del kernel para el MaxPool1d (pooling en la dimensión temporal).\n",
    "        do_prob (float): Probabilidad de dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, \n",
    "                    pool_kernel_size: int, do_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        layers.append( nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                 kernel_size=kernel_size, stride=1, \n",
    "                                 padding=\"same\", padding_mode= \"replicate\") )\n",
    "        \n",
    "        layers.append(Transpose(1,2))\n",
    "        layers.append(nn.LayerNorm(out_channels)) # LayerNorm opera sobre la última dimensión, que es out_channels\n",
    "        layers.append(Transpose(1,2))\n",
    "        \n",
    "        layers.append(nn.LeakyReLU())\n",
    "        \n",
    "        layers.append(nn.MaxPool1d(kernel_size=pool_kernel_size, stride=1))\n",
    "        layers.append(nn.Dropout(p=do_prob))\n",
    "        self.bloque_cnn = nn.Sequential(*layers)\n",
    "        self.pool_kernel_size = pool_kernel_size\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, lengths: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass para el CNN_block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor de entrada (batch, in_channels, sequence_length).\n",
    "            lengths (torch.Tensor): Tensor de longitudes originales de las secuencias (batch,).\n",
    "                                    Estas longitudes corresponden a la 'sequence_length' de 'x'.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - x_masked (torch.Tensor): Salida del bloque CNN con padding enmascarado.\n",
    "                                           Shape: (batch, out_channels, new_sequence_length)\n",
    "                - new_lengths (torch.Tensor): Longitudes actualizadas de las secuencias\n",
    "                                              después del pooling. Shape: (batch,).\n",
    "        \"\"\"\n",
    "        # Calcular las nuevas longitudes después del MaxPool1d\n",
    "        # MaxPool1d con stride=1 reduce la longitud en (kernel_size - 1)\n",
    "        new_lengths = lengths - self.pool_kernel_size + 1\n",
    "        \n",
    "        # Asegurarse de que las longitudes no sean negativas o cero (mínimo 1)\n",
    "        new_lengths = torch.clamp(new_lengths, min=1) \n",
    "\n",
    "        x = self.bloque_cnn(x)\n",
    "\n",
    "        # Crear una máscara booleana para las regiones de padding\n",
    "        output_max_len = x.size(2)\n",
    "        \n",
    "        mask = torch.arange(output_max_len, device=x.device).unsqueeze(0) < new_lengths.unsqueeze(1)\n",
    "        mask_expanded = mask.unsqueeze(1) \n",
    "        \n",
    "        x_masked = x * mask_expanded.float() # Aplicar la máscara\n",
    "\n",
    "        return x_masked, new_lengths\n",
    "\n",
    "# --- Modelo Principal CRDNN ---\n",
    "class CRDNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo tipo CRDNN (Convolutional Recurrent Deep Neural Network)\n",
    "    que integra extracción de características Mel, bloques CNN, RNN y DNN.\n",
    "    Gestiona la propagación de longitudes para el padding a lo largo de la red.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 sample_rate: int,\n",
    "                 n_fft: int,\n",
    "                 hop_length: int,\n",
    "                 n_mels: int, # Dimensión de salida del MelSpectrogram y entrada al primer CNN_block\n",
    "                 n_cnn_blocks: int, \n",
    "                 cnn_channels: list[int], # Lista de canales de salida para cada CNN_block\n",
    "                 cnn_kernel_size: int, \n",
    "                 cnn_pool_kernel_size: int, \n",
    "                 cnn_dropout: float,\n",
    "                 n_rnn_layers: int, \n",
    "                 rnn_hidden_size: int, \n",
    "                 rnn_bidirectional: bool, \n",
    "                 rnn_dropout: float,\n",
    "                 n_dnn_layers: int, \n",
    "                 dnn_hidden_size: int, \n",
    "                 dnn_dropout: float,\n",
    "                 num_classes: int # Número de clases de salida (ej. fonemas + CTC blank)\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "        # --- Extracción de Características: MelSpectrogram ---\n",
    "        # torchaudio.transforms.MelSpectrogram espera input (..., time_samples)\n",
    "        # y produce output (..., n_mels, time_frames)\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            normalized=True # Normaliza la energía para evitar valores muy grandes\n",
    "        )\n",
    "\n",
    "        # --- Bloques CNN ---\n",
    "        self.n_cnn_blocks = n_cnn_blocks\n",
    "        if not isinstance(cnn_channels, list) or len(cnn_channels) != n_cnn_blocks:\n",
    "            raise ValueError(\n",
    "                f\"cnn_channels debe ser una lista con {n_cnn_blocks} elementos, \"\n",
    "                f\"pero se recibió: {cnn_channels} (longitud {len(cnn_channels) if isinstance(cnn_channels, list) else 'N/A'})\"\n",
    "            )\n",
    "\n",
    "        cnn_modules = nn.ModuleList()\n",
    "        current_in_channels = n_mels # La entrada al primer CNN_block es la salida de MelSpectrogram\n",
    "        \n",
    "        for i in range(n_cnn_blocks):\n",
    "            out_channels = cnn_channels[i]\n",
    "            \n",
    "            cnn_modules.append(\n",
    "                CNN_block(\n",
    "                    in_channels=current_in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=cnn_kernel_size,\n",
    "                    pool_kernel_size=cnn_pool_kernel_size,\n",
    "                    do_prob=cnn_dropout\n",
    "                )\n",
    "            )\n",
    "            current_in_channels = out_channels\n",
    "        self.cnn_blocks = cnn_modules\n",
    "\n",
    "        # --- Preparación para RNN ---\n",
    "        # La salida del último CNN_block es (batch, final_cnn_channels, final_time_frames)\n",
    "        # La RNN espera (batch, final_time_frames, features_dim)\n",
    "        rnn_input_size = cnn_channels[-1] \n",
    "\n",
    "        # --- Bloques RNN (GRU en este caso) ---\n",
    "        self.n_rnn_layers = n_rnn_layers\n",
    "        rnn_modules = nn.ModuleList()\n",
    "        current_rnn_input_size = rnn_input_size\n",
    "        \n",
    "        for i in range(n_rnn_layers):\n",
    "            rnn_modules.append(\n",
    "                nn.GRU(\n",
    "                    input_size=current_rnn_input_size,\n",
    "                    hidden_size=rnn_hidden_size,\n",
    "                    num_layers=1, # Cada GRU en la lista es una sola capa\n",
    "                    batch_first=True,\n",
    "                    bidirectional=rnn_bidirectional,\n",
    "                    dropout=rnn_dropout if i < n_rnn_layers - 1 else 0 \n",
    "                )\n",
    "            )\n",
    "            current_rnn_input_size = rnn_hidden_size * 2 if rnn_bidirectional else rnn_hidden_size\n",
    "        self.rnn_blocks = rnn_modules\n",
    "\n",
    "        # --- Bloques DNN ---\n",
    "        self.n_dnn_layers = n_dnn_layers\n",
    "        dnn_modules = nn.ModuleList()\n",
    "        current_dnn_input_size = rnn_hidden_size * 2 if rnn_bidirectional else rnn_hidden_size \n",
    "        \n",
    "        for i in range(n_dnn_layers):\n",
    "            out_dnn_size = dnn_hidden_size\n",
    "            dnn_modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(current_dnn_input_size, out_dnn_size),\n",
    "                    nn.LeakyReLU(), \n",
    "                    nn.Dropout(p=dnn_dropout)\n",
    "                )\n",
    "            )\n",
    "            current_dnn_input_size = out_dnn_size\n",
    "        self.dnn_blocks = dnn_modules\n",
    "\n",
    "        # --- Capa de Salida Final ---\n",
    "        self.output_layer = nn.Linear(current_dnn_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x_waveform: torch.Tensor, lengths_waveform: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass del modelo CRDNN.\n",
    "\n",
    "        Args:\n",
    "            x_waveform (torch.Tensor): Tensor de entrada de audio crudo (batch, num_samples).\n",
    "            lengths_waveform (torch.Tensor): Tensor de longitudes originales de las muestras de audio (batch,).\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor]:\n",
    "                - logits (torch.Tensor): Salidas del modelo (batch, final_time_frames, num_classes).\n",
    "                - output_lengths (torch.Tensor): Longitudes de las secuencias de salida (batch,).\n",
    "        \"\"\"\n",
    "        # --- 1. Extracción de Características Mel-espectrograma ---\n",
    "        # x_waveform es (batch, num_samples)\n",
    "        # mel_features será (batch, n_mels, time_frames)\n",
    "        mel_features = self.mel_transform(x_waveform)\n",
    "\n",
    "        # Calcular las longitudes de los frames del Mel-espectrograma\n",
    "        # La longitud de los frames es floor((num_samples - n_fft) / hop_length) + 1\n",
    "        # O, si MelSpectrogram maneja padding, puede ser ceil(num_samples / hop_length)\n",
    "        # Usaremos la fórmula de torchaudio para calcular las longitudes de los frames:\n",
    "        # num_frames = floor( (num_samples - n_fft) / hop_length ) + 1\n",
    "        # Sin embargo, torchaudio.transforms.MelSpectrogram puede tener un comportamiento de padding\n",
    "        # que hace que la longitud sea ceil(num_samples / hop_length).\n",
    "        # Para ser robustos, usaremos la longitud real de la dimensión temporal de mel_features.\n",
    "        # Y para las longitudes, necesitamos calcular cuántos frames válidos hay.\n",
    "        \n",
    "        # Una forma común de calcular las longitudes de los frames es:\n",
    "        # mel_frame_lengths = (lengths_waveform.float() / self.hop_length).ceil().long()\n",
    "        # Esto puede ser una aproximación. La forma más precisa es usar la longitud real de la dimensión temporal\n",
    "        # y ajustar las longitudes originales de los samples.\n",
    "        \n",
    "        # La longitud de los frames después de MelSpectrogram es:\n",
    "        # (longitud_original_samples - n_fft) / hop_length + 1\n",
    "        # O, si hay `pad_to_max_len=True` en MelSpectrogram, es más complejo.\n",
    "        # Para la mayoría de los casos, la longitud de los frames se reduce por `hop_length`.\n",
    "        \n",
    "        # Calculamos la longitud de los frames válidos en el Mel-espectrograma\n",
    "        # Usamos la fórmula de torchaudio para `compute_output_shape` de un STFT\n",
    "        # num_frames = floor((num_samples - n_fft) / hop_length) + 1\n",
    "        # Si el MelSpectrogram no hace padding adicional, la longitud del frame es:\n",
    "        mel_frame_lengths = torch.floor_divide(lengths_waveform - self.mel_transform.n_fft, self.hop_length) + 1\n",
    "        mel_frame_lengths = torch.clamp(mel_frame_lengths, min=1) # Asegurar que no sea cero o negativo\n",
    "\n",
    "        # --- 2. Forward Pass de los Bloques CNN ---\n",
    "        current_x = mel_features\n",
    "        current_lengths = mel_frame_lengths\n",
    "\n",
    "        for cnn_block in self.cnn_blocks:\n",
    "            current_x, current_lengths = cnn_block(current_x, current_lengths)\n",
    "        \n",
    "        # current_x es (batch, final_cnn_channels, final_time_frames)\n",
    "        # current_lengths son las longitudes de los frames después de las CNNs\n",
    "\n",
    "        # --- 3. Preparación para RNN ---\n",
    "        # Permutar las dimensiones para que la RNN reciba (batch, time_frames, features_dim)\n",
    "        rnn_input = current_x.permute(0, 2, 1).contiguous() \n",
    "\n",
    "        # --- 4. Forward Pass de los Bloques RNN ---\n",
    "        # Empaquetar secuencias para manejar el padding en la RNN.\n",
    "        # `lengths` debe estar en CPU para `pack_padded_sequence`.\n",
    "        packed_rnn_input = pack_padded_sequence(\n",
    "            rnn_input, current_lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_rnn_output = packed_rnn_input\n",
    "        for rnn_block in self.rnn_blocks:\n",
    "            packed_rnn_output, _ = rnn_block(packed_rnn_output)\n",
    "        \n",
    "        # Desempaquetar la salida de la RNN para volver a tener un tensor acolchado\n",
    "        rnn_output, _ = pad_packed_sequence(packed_rnn_output, batch_first=True)\n",
    "        # rnn_output ahora es (batch, final_time_frames, rnn_output_features_dim)\n",
    "\n",
    "        # --- 5. Forward Pass de los Bloques DNN ---\n",
    "        dnn_input = rnn_output\n",
    "        for dnn_block in self.dnn_blocks:\n",
    "            dnn_input = dnn_block(dnn_input)\n",
    "        \n",
    "        # --- 6. Capa de Salida Final ---\n",
    "        logits = self.output_layer(dnn_input)\n",
    "\n",
    "        # Devolver los logits y las longitudes de los frames de salida (current_lengths)\n",
    "        # Estas longitudes son cruciales para el cálculo de la función de pérdida CTC.\n",
    "        return logits, current_lengths\n",
    "\n",
    "\n",
    "# --- CLASES Y FUNCIONES DE DATOS (Para que el ejemplo sea autocontenido y ejecutable) ---\n",
    "\n",
    "# --- Variables de Ruta ---\n",
    "vocab_file = 'data/label_encoder_new.txt'\n",
    "train_json = 'data/train.json'\n",
    "test_json = 'data/test.json'\n",
    "valid_json = 'data/dev.json'\n",
    "\n",
    "# --- Función para Cargar Vocabulario ---\n",
    "def load_phoneme_vocabulary(filepath: str) -> tuple[dict, dict]:\n",
    "    phoneme_to_idx = {}\n",
    "    idx_to_phoneme = {}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('=>')\n",
    "            if len(parts) == 2:\n",
    "                phoneme = parts[0].strip().strip(\"'\")\n",
    "                idx_str = parts[1].strip()\n",
    "                try:\n",
    "                    index = int(idx_str)\n",
    "                    phoneme_to_idx[phoneme] = index\n",
    "                    idx_to_phoneme[index] = phoneme\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"Error: Índice inválido en la línea: '{line.strip()}'\")\n",
    "            else:\n",
    "                raise ValueError(f\"Error: Línea mal formada (se esperaba 'fonema=>indice'): '{line.strip()}'\")\n",
    "    return phoneme_to_idx, idx_to_phoneme\n",
    "\n",
    "# --- Clase TimitDataset ---\n",
    "class TimitDataset(Dataset):\n",
    "    def __init__(self, json_file: str, vocab_file: str):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                self.datos_json = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Error: El archivo JSON de datos '{json_file}' no se encuentra. Asegúrate de que la ruta sea correcta.\")\n",
    "        except json.JSONDecodeError:\n",
    "            raise ValueError(f\"Error: El archivo '{json_file}' no es un JSON válido.\")\n",
    "        \n",
    "        self.datos_ids = list(self.datos_json.keys())\n",
    "        \n",
    "        try:\n",
    "            self.str2int, self.int2str = load_phoneme_vocabulary(vocab_file)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Error: El archivo de vocabulario '{vocab_file}' no se encuentra. Asegúrate de que la ruta sea correcta.\")\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error al cargar el vocabulario: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datos_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        key = self.datos_ids[idx]\n",
    "        wav_path = self.datos_json[key]['wav']\n",
    "        phn_text = self.datos_json[key]['phn']\n",
    "\n",
    "        try:\n",
    "            waveform, _ = torchaudio.load(wav_path)\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Error: Archivo de audio '{wav_path}' no encontrado para la muestra '{key}'.\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error al cargar el audio '{wav_path}' para la muestra '{key}': {e}\")\n",
    "\n",
    "        if waveform.ndim > 1 and waveform.shape[0] == 1:\n",
    "            waveform = waveform.squeeze(0) # Asegurarse de que sea 1D (samples,)\n",
    "\n",
    "        phn_list = [p for p in phn_text.strip().split() if p] \n",
    "        phn_indices = [self.str2int[phoneme] for phoneme in phn_list if phoneme in self.str2int]\n",
    "        \n",
    "        if not phn_indices and phn_list:\n",
    "            print(f\"Advertencia: No se encontraron índices válidos para los fonemas en '{key}': {phn_list}\")\n",
    "            phn_tensor = torch.tensor([], dtype=torch.long)\n",
    "        else:\n",
    "            phn_tensor = torch.tensor(phn_indices, dtype=torch.long)\n",
    "        \n",
    "        return waveform, phn_tensor # Devolvemos waveform (raw audio) y phn_tensor\n",
    "\n",
    "# --- Función collate_fn para DataLoader ---\n",
    "def collate_fn(batch: list[tuple[torch.Tensor, torch.Tensor]]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Función de agrupación (collate_fn) para el DataLoader.\n",
    "    Acolcha secuencias de audio y etiquetas, y devuelve sus longitudes originales\n",
    "    y sus longitudes normalizadas (relativas a la longitud máxima del batch).\n",
    "    \"\"\"\n",
    "    # Desempaquetar el batch: waveforms son audio crudo, phn_tensors son etiquetas de fonemas\n",
    "    waveforms, phn_tensors = zip(*batch)\n",
    "\n",
    "    # --- Procesar Waveforms (audio crudo) ---\n",
    "    # 1. Longitudes originales de las waveforms (en número de samples)\n",
    "    waveform_lengths_orig = torch.tensor([w.shape[0] for w in waveforms], dtype=torch.long)\n",
    "    \n",
    "    # 2. Acolchar waveforms\n",
    "    padded_waveforms = pad_sequence(waveforms, batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    # 3. Longitud máxima del batch para waveforms (después de acolchar)\n",
    "    max_waveform_len = padded_waveforms.shape[1]\n",
    "    \n",
    "    # 4. Longitudes normalizadas (relativas a la longitud máxima del batch)\n",
    "    waveform_lengths_norm = waveform_lengths_orig.float() / max_waveform_len\n",
    "\n",
    "    # --- Procesar Etiquetas de Fonemas ---\n",
    "    # 1. Longitudes originales de las etiquetas de fonemas\n",
    "    phn_lengths_orig = torch.tensor([p.shape[0] for p in phn_tensors], dtype=torch.long)\n",
    "    \n",
    "    # 2. Acolchar etiquetas de fonemas\n",
    "    # Se recomienda usar un padding_value que no sea un índice de fonema real si es posible.\n",
    "    # Aquí, usaremos 0 por simplicidad, asumiendo que es el ID de 'sil' o un token de padding.\n",
    "    padded_phns = pad_sequence(phn_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 3. Longitud máxima del batch para fonemas\n",
    "    max_phn_len = padded_phns.shape[1]\n",
    "    \n",
    "    # 4. Longitudes normalizadas (relativas a la longitud máxima del batch)\n",
    "    phn_lengths_norm = phn_lengths_orig.float() / max_phn_len\n",
    "\n",
    "    # Retorna todas las versiones: acolchadas, longitudes originales, longitudes normalizadas\n",
    "    return padded_waveforms, waveform_lengths_orig, waveform_lengths_norm, \\\n",
    "           padded_phns, phn_lengths_orig, phn_lengths_norm\n",
    "\n",
    "\n",
    "# --- Ejemplo de Uso Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuración de Parámetros Dummy ---\n",
    "    # Estos serían tus hparams cargados de un YAML\n",
    "    hparams_example = {\n",
    "        \"sample_rate\": 16000,\n",
    "        \"n_fft\": 400,       # 25ms window at 16kHz\n",
    "        \"hop_length\": 160,  # 10ms hop at 16kHz\n",
    "        \"n_mels\": 80,       # Número de Mel-bins\n",
    "        \"n_cnn_blocks\": 2,\n",
    "        \"cnn_channels\": [64, 128], # Canales de salida para cada CNN block\n",
    "        \"cnn_kernel_size\": 3,\n",
    "        \"cnn_pool_kernel_size\": 2, # Pooling en la dimensión temporal (reduce longitud en 1 por bloque)\n",
    "        \"cnn_dropout\": 0.15,\n",
    "        \"n_rnn_layers\": 2,\n",
    "        \"rnn_hidden_size\": 256,\n",
    "        \"rnn_bidirectional\": True,\n",
    "        \"rnn_dropout\": 0.15,\n",
    "        \"n_dnn_layers\": 3,\n",
    "        \"dnn_hidden_size\": 512,\n",
    "        \"dnn_dropout\": 0.15,\n",
    "        \"num_classes\": 40 # Ejemplo: 39 fonemas + 1 para el blank de CTC\n",
    "    }\n",
    "\n",
    "    # --- Preparación de Archivos Dummy para Ejecución ---\n",
    "    # Crea la carpeta 'data' si no existe\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    dummy_wav_dir = 'data/dummy_wavs'\n",
    "    if not os.path.exists(dummy_wav_dir):\n",
    "        os.makedirs(dummy_wav_dir)\n",
    "\n",
    "    def create_dummy_wav(path, duration_samples, sr=16000):\n",
    "        dummy_audio = torch.randn(1, duration_samples) * 0.1 # Pequeño volumen\n",
    "        torchaudio.save(path, dummy_audio, sr)\n",
    "\n",
    "    # Crea un vocabulario dummy\n",
    "    with open(vocab_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"sil=>0\\n\")\n",
    "        f.write(\"sp=>1\\n\")\n",
    "        f.write(\"aa=>2\\n\")\n",
    "        f.write(\"ae=>3\\n\")\n",
    "        f.write(\"z=>4\\n\")\n",
    "        f.write(\"ah=>5\\n\")\n",
    "        f.write(\"bh=>6\\n\")\n",
    "        f.write(\"ch=>7\\n\")\n",
    "        f.write(\"dh=>8\\n\")\n",
    "        f.write(\"eh=>9\\n\")\n",
    "        f.write(\"er=>10\\n\")\n",
    "        f.write(\"f=>11\\n\")\n",
    "        f.write(\"g=>12\\n\")\n",
    "        f.write(\"hh=>13\\n\")\n",
    "        f.write(\"ih=>14\\n\")\n",
    "        f.write(\"iy=>15\\n\")\n",
    "        f.write(\"jh=>16\\n\")\n",
    "        f.write(\"k=>17\\n\")\n",
    "        f.write(\"l=>18\\n\")\n",
    "        f.write(\"m=>19\\n\")\n",
    "        f.write(\"n=>20\\n\")\n",
    "        f.write(\"ng=>21\\n\")\n",
    "        f.write(\"ow=>22\\n\")\n",
    "        f.write(\"oy=>23\\n\")\n",
    "        f.write(\"p=>24\\n\")\n",
    "        f.write(\"r=>25\\n\")\n",
    "        f.write(\"s=>26\\n\")\n",
    "        f.write(\"sh=>27\\n\")\n",
    "        f.write(\"t=>28\\n\")\n",
    "        f.write(\"th=>29\\n\")\n",
    "        f.write(\"uh=>30\\n\")\n",
    "        f.write(\"uw=>31\\n\")\n",
    "        f.write(\"v=>32\\n\")\n",
    "        f.write(\"w=>33\\n\")\n",
    "        f.write(\"y=>34\\n\")\n",
    "        f.write(\"zh=>35\\n\")\n",
    "        f.write(\"dx=>36\\n\")\n",
    "        f.write(\"nx=>37\\n\")\n",
    "        f.write(\"eng=>38\\n\")\n",
    "        f.write(\"blank=>39\\n\") # Para el token blank de CTC\n",
    "\n",
    "    # Crea archivos JSON dummy para train, test, dev\n",
    "    # Asegúrate de que las duraciones sean suficientes para que los audios no sean demasiado cortos\n",
    "    # para el n_fft y hop_length definidos.\n",
    "    sample_data = {\n",
    "        \"sample_001\": {\"wav\": os.path.join(dummy_wav_dir, \"audio_001.wav\"), \"duration\": 1.5, \"phn\": \"aa sp ae z\"}, # 1.5s * 16000 = 24000 samples\n",
    "        \"sample_002\": {\"wav\": os.path.join(dummy_wav_dir, \"audio_002.wav\"), \"duration\": 2.0, \"phn\": \"sil aa ae ah\"}, # 2.0s * 16000 = 32000 samples\n",
    "        \"sample_003\": {\"wav\": os.path.join(dummy_wav_dir, \"audio_003.wav\"), \"duration\": 0.8, \"phn\": \"z\"}, # 0.8s * 16000 = 12800 samples\n",
    "        \"sample_004\": {\"wav\": os.path.join(dummy_wav_dir, \"audio_004.wav\"), \"duration\": 2.5, \"phn\": \"aa ae sp sil ah z\"}, # 2.5s * 16000 = 40000 samples\n",
    "    }\n",
    "\n",
    "    for key, val in sample_data.items():\n",
    "        wav_path = val['wav']\n",
    "        duration_samples = int(val['duration'] * hparams_example[\"sample_rate\"])\n",
    "        create_dummy_wav(wav_path, duration_samples, hparams_example[\"sample_rate\"])\n",
    "\n",
    "    with open(train_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_data, f, indent=4)\n",
    "    with open(test_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_data, f, indent=4)\n",
    "    with open(valid_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sample_data, f, indent=4)\n",
    "\n",
    "    print(\"Archivos dummy creados para la demostración.\\n\")\n",
    "\n",
    "    # --- Instanciar y Probar el Modelo CRDNN ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Usando dispositivo: {device}\\n\")\n",
    "\n",
    "    model = CRDNNModel(**hparams_example).to(device)\n",
    "    print(\"Estructura del Modelo CRDNN:\")\n",
    "    print(model)\n",
    "\n",
    "    print(\"\\n--- Probando un batch a través del modelo ---\")\n",
    "    try:\n",
    "        train_ds = TimitDataset(train_json, vocab_file)\n",
    "        train_dl = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "        for i, (waveforms, wf_lengths_orig, wf_lengths_norm, phns, phn_lengths_orig, phn_lengths_norm) in enumerate(train_dl):\n",
    "            # Mover datos al dispositivo\n",
    "            waveforms = waveforms.to(device)\n",
    "            wf_lengths_orig = wf_lengths_orig.to(device)\n",
    "            # phns y phn_lengths_orig/norm se usarían en la función de pérdida (CTC), no en el forward del modelo\n",
    "\n",
    "            # Pasar los waveforms y sus longitudes originales al modelo\n",
    "            logits, output_lengths = model(waveforms, wf_lengths_orig)\n",
    "\n",
    "            print(f\"Batch {i+1}:\")\n",
    "            print(f\"  Input Waveforms Shape: {waveforms.shape}\")\n",
    "            print(f\"  Input Waveform Lengths (Original): {wf_lengths_orig}\")\n",
    "            print(f\"  Logits Shape (salida del modelo): {logits.shape}\")\n",
    "            print(f\"  Output Lengths (después de Mel, CNNs y RNNs): {output_lengths}\")\n",
    "            print(\"-\" * 30)\n",
    "            if i == 0: # Solo mostrar el primer batch para brevedad\n",
    "                break\n",
    "        \n",
    "        print(\"\\n¡Modelo CRDNN integrado y probado con éxito!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcurrió un error durante la ejecución del ejemplo del modelo: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a067e5",
   "metadata": {},
   "source": [
    "#### Iteraciones con tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ab80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "i = 0\n",
    "for batch in tqdm.tqdm(train_dl,colour=\"yellow\",desc='entrenando'):\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
